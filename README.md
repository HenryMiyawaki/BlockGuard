# Detec√ß√£o de Ataques em Blockchain com Aprendizado de M√°quina

Status: ‚úÖ Completo

![](https://img.shields.io/badge/Code-Python-informational?style=for-the-badge&logo=python&logoColor=white&color=BD2A95)

## 1Ô∏è‚É£ Introdu√ß√£o

As redes e sistemas descentralizados como blockchain, t√™m ganhado destaque como uma alternativa eficiente e inovadora em um mundo cada vez mais preocupado com privacidade e autonomia ao se destacarem por: 

> - Eliminar a depend√™ncia de uma entidade central
> - Colaboratividade e Escalabilidade da rede
> - Seguran√ßa e Imutabilidade

## 2Ô∏è‚É£ Objetivos

Propomos, assim, um modelo que com os seguintes objetivos

> - ‚úÖ Identifique tipos de ataques comuns em Blockchain.
>   - Brute Force
>   - Denial of Service (Dos)
>   - Flooding of Transactions
>   - Man-in-the-Middle
> - ‚úÖ O modelo pode aprender com outros n√≥s da rede.
> - ‚úÖ Aprendizado descentralizada sem expor os dados dos peers.

## 3Ô∏è‚É£ Metodologia 

Foi delimitado nossa problem√°tica como um problema de **Classifica√ß√£o** onde seria necess√°rio a partir de vari√°veis independentes classificar uma certa requisi√ß√£o em 4 categorias.

### üìà Dataset

O dataset escolhido foi o BNaT Dataset (Blockchain Network Attack Traffic Dataset) uma base de dados coletados de n√≥s **Ethereum** que cont√™m os ataques listados nos objetivos.

-> [Conhe√ßa o BNaT Dataset](https://paperswithcode.com/dataset/bnat)

### üíª Modelo

Para resolver o problema de classifica√ß√£o foi escolhido criar um modelo de **Rede Neural** devido a quantidade de itens para serem classificados, flexibilidade que uma rede neural pode oferecer.

> Foi utilizado as seguintes ferramentas para a cria√ß√£o da rede: 
> - Python 3.12.4
> - Pytorch
> - Scikit-learn
> - Numpy
> - Pandas

### üìä Pr√©-processamento

> Para melhorar a qualidade e divis√£o dos dados, foi adotada as seguintes t√©cnicas: 
> - Estratifica√ß√£o para a separa√ß√£o dos conjuntos de treino e teste
> - **Transforma√ß√£o** matrizes de m√∫ltiplas dimens√µes para **vectorized operation**
> - Carregamento dos dados em lotes (batches embaralhados de 32 amostras)

### üß† Rede Neural

A rede neural foi constru√≠da da seguinte forma:

#### **3.1 Topologia**
> - A primeira camada √© totalmente conectada (linear) e transforma a entrada de dimens√£o N em uma sa√≠da de 128 Neur√¥nios.
> - A segunda camada tamb√©m totalmente conectada por√©m reduz de 128 neur√¥nios para 64.
> - A terceira camada totalmente conectada reduz a dimens√£o de 64 para 32 neur√¥nios.
> - Por fim, a camada de sa√≠da reduz os 32 neur√¥nios para o n√∫mero de classes do problema de classifica√ß√£o.

Todas as camadas seguem os seguintes passos:
> 1. Os dados passam pela camada linear
> 2. Sofrem uma normaliza√ß√£o em lotes
> 3. √â aplicado a fun√ß√£o de ativa√ß√£o ReLU (Rectified Linear Unit)

#### **3.2 Fun√ß√£o de Perda e Otimizador**
```diff
- Para calcular a perda da rede foi utilizado o Cross-Entropy-Loss. 
+ Para otimizar o modelo foi escolhido o Adaptive Moment Estimation (ADAM).
+ Scheduler foi usado para ajustar dinamicamente a taxa de aprendizado.
- Caso 3 √©pocas consecutivas sem progresso √© aplicado um fator de 0.5 na taxa de aprendizado.
```
***

| üíæ Taxa de aprendizado       | üìêRegulariza√ß√£o   |
| -----------                  | -----------        |
| 0.001                        | **L1 (Lasso)**     |


## 4Ô∏è‚É£ Treinamento

> Para o processo de treinamento foi definido um mecanismo de **Early Stopping** que interrompe o treinamento caso o modelo n√£o apresente melhorias consecutivas em um determinado n√∫mero de √©pocas.

Durante o processo de treino funcionalidades espec√≠ficas s√£o ativadas usadas como **dropout** e **normaliza√ß√£o**
1. -> Para cada itera√ß√£o moveremos os dados do batch para o dispositivo para a CPU ou GPU (CUDA)
2. -> Calcula-se a sa√≠da do modelo e a perda associada
3. -> √â **zerado** os gradientes acumulados
4. -> Utilizamos as perdas calculadas para calcular o gradiente dos par√¢metros do modelo.
5. -> Calculamos a perda do lote e a perda m√©dia das √©pocas
6. -> Por fim √© verificado a condi√ß√£o de **Early Stopping**

## 5Ô∏è‚É£ Peer Network
> O objetivo central do sistema de Peer Network √© criar uma rede descentralizada onde cada n√≥ (peer) possui um modelo local, que pode ser treinado e atualizado com base em dados sint√©ticos e intera√ß√µes com outros peers, o sistema √© projetado para.
> - Promover um aprendizado colaborativo
> - Utilizar o conceito de **Knowledge Distillation**
>   - Cada peer pode atuar como **teacher** quanto como **student**
> - Aprimorar o desempenho do modelo de forma descentralizada sem depender de um servidor central (Federated Learning) 

### Passos da Metodologia do Peer-Network
> 1. Gera√ß√£o de Dados Sint√©ticos Locais
>    - Utiliza√ß√£o de datasets aleat√≥rios para cada peer.
> 3. Treinamento Local do Modelo
>    - Feito maneira de aut√¥noma
>    - Durante essa fase os peers n√£o compartilham dados
> 5. Destila√ß√£o de Conhecimento (Knowledge Destilation)
>    - üë©üèø‚Äçüè´ Teacher (Modelo mais complexo) fornece feedback para o modelo menos treinado
>    - üßëüèª‚Äçüéì Student (Modelo menos complexo) aprende com as previs√µes e par√¢metros do Teacher
> 7. Evolu√ß√£o e Atualiza√ß√£o
>    - Ap√≥s a atualiza√ß√£o modelo √© permitido que o conhecimento se propague pela rede
> 9. Valida√ß√£o de Teste Descentralizada
>    - Para garantir que a evolu√ß√£o dos modelos seja efetiva, cada peer realiza testes de valida√ß√£o de forma local
> 11. Ajuste de Par√¢metros e Estrat√©gias de Colabora√ß√£o
>     - Os peers podem adotar estrat√©gias como a sele√ß√£o de modelos de Teacher com base no desempenho ou at√© mesmo na complexidade do modelo.
>     - Os peers podem decidir quando compartilhar seu modelo com outros peers, ou quando evoluir de forma isolada.


## 6Ô∏è‚É£ Vantagens do Peer Network
As seguintes vantagens podem ser alcan√ßadas com o Peer-Network sendo elas:
```diff
+ Descentraliza√ß√£o com o sistema operando de forma distribu√≠da.

+ Aprendizado colaborativo ao se utilizar do conceito de Teacher e Student.

+ Valida√ß√£o Cont√≠nua que permite que cada peer avalie e melhore o desempenho do seu modelo.

+ Evolu√ß√£o de conhecimento com base nas intera√ß√µes entre os peers permitindo a adaptabilidade a novos dados que os peers s√£o expostos.

+ Anonimato e prote√ß√£o dos dados
```

## 7Ô∏è‚É£ Resultados

Os resultados obtidos podem ser observados nos seguintes gr√°ficos.

> ### ROC Curve
> <img src="/results/roc_curve.png" alt="Descri√ß√£o da imagem" width="600">
> 
> ### Class Distribution
> <img src="/results/class_distribution.png" alt="Descri√ß√£o da imagem" width="600">
> 
> ### Confusion Matrix
> <img src="/results/confusion_matrix.png" alt="Descri√ß√£o da imagem" width="600">

# Contribuidores
- [Felipe Nunes Melo](https://github.com/felipemelonunes09)
- [Henry Miyawaki](https://github.com/HenryMiyawaki)
- 

[V√≠deo Demonstra√ß√£o](https://drive.google.com/file/d/12Y5XDmX9jIcxaA3Or7tud8SfSUvhmF5c/view?usp=drive_link)
