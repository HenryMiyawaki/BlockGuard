# DetecÃ§Ã£o de Ataques em Blockchain com Aprendizado de MÃ¡quina

Status: âœ… Completo

![](https://img.shields.io/badge/Code-Python-informational?style=for-the-badge&logo=python&logoColor=white&color=BD2A95)

## 1ï¸âƒ£ IntroduÃ§Ã£o

As redes e sistemas descentralizados como blockchain, tÃªm ganhado destaque como uma alternativa eficiente e inovadora em um mundo cada vez mais preocupado com privacidade e autonomia ao se destacarem por: 

> - Eliminar a dependÃªncia de uma entidade central
> - Colaboratividade e Escalabilidade da rede
> - SeguranÃ§a e Imutabilidade

## 2ï¸âƒ£ Objetivos

Propomos, assim, um modelo que com os seguintes objetivos

> - âœ… Identifique tipos de ataques comuns em Blockchain.
>   - Brute Force
>   - Denial of Service (Dos)
>   - Flooding of Transactions
>   - Man-in-the-Middle
> - âœ… O modelo pode aprender com outros nÃ³s da rede.
> - âœ… Aprendizado descentralizada sem expor os dados dos peers.

## 3ï¸âƒ£ Metodologia 

Foi delimitado nossa problemÃ¡tica como um problema de **ClassificaÃ§Ã£o** onde seria necessÃ¡rio a partir de variÃ¡veis independentes classificar uma certa requisiÃ§Ã£o em 4 categorias.

### ðŸ“ˆ Dataset

O dataset escolhido foi o BNaT Dataset (Blockchain Network Attack Traffic Dataset) uma base de dados coletados de nÃ³s **Ethereum** que contÃªm os ataques listados nos objetivos.

-> [ConheÃ§a o BNaT Dataset](https://paperswithcode.com/dataset/bnat)

### ðŸ’» Modelo

Para resolver o problema de classificaÃ§Ã£o foi escolhido criar um modelo de **Rede Neural** devido a quantidade de itens para serem classificados, flexibilidade que uma rede neural pode oferecer.

> Foi utilizado as seguintes ferramentas para a criaÃ§Ã£o da rede: 
> - Python 3.12.4
> - Pytorch
> - Scikit-learn
> - Numpy
> - Pandas

### ðŸ“Š PrÃ©-processamento

> Para melhorar a qualidade e divisÃ£o dos dados, foi adotada as seguintes tÃ©cnicas: 
> - EstratificaÃ§Ã£o para a separaÃ§Ã£o dos conjuntos de treino e teste
> - **TransformaÃ§Ã£o** matrizes de mÃºltiplas dimensÃµes para **vectorized operation**
> - Carregamento dos dados em lotes (batches embaralhados de 32 amostras)

### ðŸ§  Rede Neural

A rede neural foi construÃ­da da seguinte forma:

#### **3.1 Topologia**
> - A primeira camada Ã© totalmente conectada (linear) e transforma a entrada de dimensÃ£o N em uma saÃ­da de 128 NeurÃ´nios.
> - A segunda camada tambÃ©m totalmente conectada porÃ©m reduz de 128 neurÃ´nios para 64.
> - A terceira camada totalmente conectada reduz a dimensÃ£o de 64 para 32 neurÃ´nios.
> - Por fim, a camada de saÃ­da reduz os 32 neurÃ´nios para o nÃºmero de classes do problema de classificaÃ§Ã£o.

Todas as camadas seguem os seguintes passos:
> 1. Os dados passam pela camada linear
> 2. Sofrem uma normalizaÃ§Ã£o em lotes
> 3. Ã‰ aplicado a funÃ§Ã£o de ativaÃ§Ã£o ReLU (Rectified Linear Unit)

#### **3.2 FunÃ§Ã£o de Perda e Otimizador**
```diff
- Para calcular a perda da rede foi utilizado o Cross-Entropy-Loss. 
+ Para otimizar o modelo foi escolhido o Adaptive Moment Estimation (ADAM).
+ Scheduler foi usado para ajustar dinamicamente a taxa de aprendizado.
- Caso 3 Ã©pocas consecutivas sem progresso Ã© aplicado um fator de 0.5 na taxa de aprendizado.
```
***

| ðŸ’¾ Taxa de aprendizado       | ðŸ“RegularizaÃ§Ã£o   |
| -----------                  | -----------        |
| 0.001                        | **L1 (Lasso)**     |


## 4ï¸âƒ£ Treinamento

> Para o processo de treinamento foi definido um mecanismo de **Early Stopping** que interrompe o treinamento caso o modelo nÃ£o apresente melhorias consecutivas em um determinado nÃºmero de Ã©pocas.

Durante o processo de treino funcionalidades especÃ­ficas sÃ£o ativadas usadas como **dropout** e **normalizaÃ§Ã£o**
1. -> Para cada iteraÃ§Ã£o moveremos os dados do batch para o dispositivo para a CPU ou GPU (CUDA)
2. -> Calcula-se a saÃ­da do modelo e a perda associada
3. -> Ã‰ **zerado** os gradientes acumulados
4. -> Utilizamos as perdas calculadas para calcular o gradiente dos parÃ¢metros do modelo.
5. -> Calculamos a perda do lote e a perda mÃ©dia das Ã©pocas
6. -> Por fim Ã© verificado a condiÃ§Ã£o de **Early Stopping**

## 5ï¸âƒ£ Peer Network
> O objetivo central do sistema de Peer Network Ã© criar uma rede descentralizada onde cada nÃ³ (peer) possui um modelo local, que pode ser treinado e atualizado com base em dados sintÃ©ticos e interaÃ§Ãµes com outros peers, o sistema Ã© projetado para.
> - Promover um aprendizado colaborativo
> - Utilizar o conceito de **Knowledge Distillation**
>   - Cada peer pode atuar como **teacher** quanto como **student**
> - Aprimorar o desempenho do modelo de forma descentralizada sem depender de um servidor central (Federated Learning) 

### Passos da Metodologia do Peer-Network
> 1. GeraÃ§Ã£o de Dados SintÃ©ticos Locais
>    - UtilizaÃ§Ã£o de datasets aleatÃ³rios para cada peer.
> 3. Treinamento Local do Modelo
>    - Feito maneira de autÃ´noma
>    - Durante essa fase os peers nÃ£o compartilham dados
> 5. DestilaÃ§Ã£o de Conhecimento (Knowledge Destilation)
>    - ðŸ‘©ðŸ¿â€ðŸ« Teacher (Modelo mais complexo) fornece feedback para o modelo menos treinado
>    - ðŸ§‘ðŸ»â€ðŸŽ“ Student (Modelo menos complexo) aprende com as previsÃµes e parÃ¢metros do Teacher
> 7. EvoluÃ§Ã£o e AtualizaÃ§Ã£o
>    - ApÃ³s a atualizaÃ§Ã£o modelo Ã© permitido que o conhecimento de propague pela rede
> 9. ValidaÃ§Ã£o de Teste Descentralizada
>    - Para garantir que a evoluÃ§Ã£o dos modelos seja efetiva, cada peer realiza testes de validaÃ§Ã£o de forma local
> 11. Ajuste de ParÃ¢metros e EstratÃ©gias de ColaboraÃ§Ã£o
>     - Os peers podem adotar estratÃ©gias como a seleÃ§Ã£o de modelos de Teacher com base no desempenho ou atÃ© mesmo na complexidade do modelo.
>     - Os peers podem decidir quando compartilhar seu modelo com outros peers, ou quando evoluir de forma isolada.


## 6ï¸âƒ£ Vantagens do Peer Network
As seguintes vantagens podem ser alcanÃ§adas com o Peer-Network sendo elas:
```diff
+ DescentralizaÃ§Ã£o com o sistema operando de forma distribuÃ­da.

+ Aprendizado colaborativo ao se utilizar do conceito de Teacher e Student.

+ ValidaÃ§Ã£o ContÃ­nua que permite que cada peer avalie e melhore o desempenho do seu modelo.

+ EvoluÃ§Ã£o de conhecimento com base nas interaÃ§Ãµes entre os peers permitindo a adaptabilidade a novos dados que os peers sÃ£o expostos.
```

## 7ï¸âƒ£ Resultados

Os resultados obtidos podem ser observados nos seguintes grÃ¡ficos.

> ### ROC Curve
> <img src="/results/roc_curve.png" alt="DescriÃ§Ã£o da imagem" width="600">
> 
> ### Class Distribution
> <img src="/results/class_distribution.png" alt="DescriÃ§Ã£o da imagem" width="600">
> 
> ### Confusion Matrix
> <img src="/results/confusion_matrix.png" alt="DescriÃ§Ã£o da imagem" width="600">

# Contribuidores
- [Felipe Nunes Melo](https://github.com/felipemelonunes09)
- [Henry Miyawaki](https://github.com/HenryMiyawaki)
